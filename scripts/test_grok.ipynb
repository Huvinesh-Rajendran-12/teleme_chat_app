{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from http import HTTPStatus\n",
    "import json \n",
    "import dashscope\n",
    "from openai import AsyncOpenAI\n",
    "from typing import List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI_API_KEY = os.getenv(\"XAI_API_KEY\", \"\")\n",
    "XAI_BASE_URL = os.getenv(\"XAI_BASE_URL\", \"\")\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\", \"\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=XAI_BASE_URL,\n",
    "    api_key=XAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str, unit: str = \"fahrenheit\"):\n",
    "    temperature: int\n",
    "    if unit.lower() == \"fahrenheit\":\n",
    "        temperature = 59\n",
    "    elif unit.lower() == \"celsius\":\n",
    "        temperature = 15\n",
    "    else:\n",
    "        raise ValueError(\"unit must be one of fahrenheit or celsius\")\n",
    "    return {\"location\": location, \"temperature\": temperature, \"unit\": \"fahrenheit\"}\n",
    "\n",
    "\n",
    "def get_current_ceiling(location: str):\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"ceiling\": 15000,\n",
    "        \"ceiling_type\": \"broken\",\n",
    "        \"unit\": \"ft\",\n",
    "    }\n",
    "\n",
    "tools_map = {\n",
    "    \"get_current_temperature\": get_current_temperature,\n",
    "    \"get_current_ceiling\": get_current_ceiling,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "\n",
    "dashscope.base_http_api_url = 'https://dashscope-intl.aliyuncs.com/api/v1'\n",
    "\n",
    "def embed_with_str(query: str):\n",
    "    resp = dashscope.TextEmbedding.call(\n",
    "        model=dashscope.TextEmbedding.Models.text_embedding_v3,\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        input=query)\n",
    "    if resp.status_code == HTTPStatus.OK:\n",
    "        return resp.output['embeddings'][0]['embedding']\n",
    "    else:\n",
    "        print(resp)\n",
    "\n",
    "def search_knowledge_base(query: str) -> Tuple[List[Dict[str, Any]], str]:\n",
    "    \"\"\"\n",
    "    Search the knowledge base for relevant information.\n",
    "\n",
    "    Args:\n",
    "        query: str -> query used to retrieve the relevant information.\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        1. List of the formatted results with title, content preview, and source link\n",
    "        2. String joining all relevant information from results\n",
    "    \"\"\"\n",
    "\n",
    "    embed = embed_with_str(query)\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"knowledge_base_collection\",\n",
    "        query=embed,\n",
    "        with_payload=True,\n",
    "        limit=3,\n",
    "        score_threshold=0.6\n",
    "    )\n",
    "    formatted_results = []\n",
    "    combined_text_parts = []\n",
    "    for result in results.points:\n",
    "        # Format dictionary result\n",
    "        content_preview = result.payload[\"content\"][:200] + \"...\" if len(result.payload[\"content\"]) > 200 else result.payload[\"content\"]\n",
    "        formatted_results.append({\n",
    "            \"title\": result.payload[\"title\"],\n",
    "            \"content_preview\": content_preview,\n",
    "            \"source_link\": result.payload[\"source_link\"],\n",
    "            \"relevance_score\": round(result.score, 3)\n",
    "        })\n",
    "\n",
    "        # Add to combined text\n",
    "        combined_text_parts.append(\n",
    "            f\"Title: {result.payload['title']}\\n\"\n",
    "            f\"Content: {result.payload['content']}\\n\"\n",
    "            f\"Source: {result.payload['source_link']}\\n\"\n",
    "        )\n",
    "\n",
    "    combined_text = \"\\n\".join(combined_text_parts)\n",
    "    return formatted_results, combined_text\n",
    "\n",
    "def search_doctors(query: str) -> Tuple[List[Dict[str, Any]], str]:\n",
    "    \"\"\"\n",
    "    Search for doctors based on query.\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        1. List of formatted results with doctor information\n",
    "        2. String joining all relevant information from results\n",
    "    \"\"\"\n",
    "    embed = embed_with_str(query)\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"doctor_collection\",\n",
    "        query=embed,\n",
    "        with_payload=True,\n",
    "        limit=3,\n",
    "        score_threshold=0.5,\n",
    "    )\n",
    "\n",
    "    formatted_results = []\n",
    "    combined_text_parts = []\n",
    "\n",
    "    for result in results.points:\n",
    "        # Format dictionary result\n",
    "        formatted_results.append({\n",
    "            \"doctor_name\": result.payload[\"doctor_name\"],\n",
    "            \"specialization\": result.payload[\"doctor_field\"],\n",
    "            \"description\": result.payload[\"doctor_description\"],\n",
    "            \"availability_status\": \"Available\" if result.payload[\"availability\"] else \"Not Available\",\n",
    "            \"appointment_link\": result.payload[\"appointment_link\"],\n",
    "            \"relevance_score\": round(result.score, 3)\n",
    "        })\n",
    "\n",
    "        # Add to combined text\n",
    "        combined_text_parts.append(\n",
    "            f\"Specialization: {result.payload['doctor_field']}\\n\"\n",
    "            f\"Description: {result.payload['doctor_description']}\\n\"\n",
    "        )\n",
    "\n",
    "    combined_text = \"\\n\".join(combined_text_parts)\n",
    "    return formatted_results, combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_definition = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_knowledge_base\",\n",
    "            \"description\": \"Retrieve health information from the knowledge base given the query.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The user query, e.g. what is diabetes ?\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_doctors\",\n",
    "            \"description\": \"Get the suitalbe doctors based on the user's query.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The user query, e.g. recommend me a doctor for diabetes\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "tools_map = {\n",
    "    \"search_knowledge_base\": search_knowledge_base,\n",
    "    \"search_doctors\": search_doctors,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global messages\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hi, what is acne ?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def get_next_questions(last_message: str):\n",
    "    \"\"\"Based on the last message get suggestions for follow up questions.\"\"\"\n",
    "    try:\n",
    "        prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Based on this health-related response: \"{last_message}\"\n",
    "            Suggest 2 natural follow-up questions that a patient might want to ask.\n",
    "            These should be within 4 four words.\n",
    "            Return ONLY the questions in a Python list format like this: [\"question 1\", \"question 2\"]\n",
    "            The questions should be clear and concise.\"\"\"\n",
    "        }\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"grok-2-1212\",\n",
    "            messages=[prompt],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        suggestions_text = response.choices[0].message.content\n",
    "        import ast\n",
    "        try:\n",
    "            suggestions = ast.literal_eval(suggestions_text)\n",
    "            if isinstance(suggestions, list) and len(suggestions) > 0:\n",
    "                return suggestions[:2]  # Ensure we only get 2 questions\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return [\"Could you explain more about that?\",\n",
    "                \"Summarize this\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating suggestions: {e}\")\n",
    "        return [\"Could you explain more about that?\",\n",
    "                \"Summarize this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am calling the `search_knowledge_base` function with the query 'what is acne' to provide you with more detailed information on acne.\n",
      "\n",
      "I've provided you with some information on acne and how to manage it. If you have any more questions about acne or need further assistance, feel free to ask!\n",
      "\n",
      "Error generating suggestions: 'coroutine' object has no attribute 'choices'\n",
      "suggestions ['Could you explain more about that?', 'Summarize this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/xbklj4xx5v7gtr23mxypjc580000gp/T/ipykernel_20265/2488641057.py:61: RuntimeWarning: coroutine 'AsyncCompletions.create' was never awaited\n",
      "  suggestions = await get_next_questions(messages[-1]['content'])\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/var/folders/3w/xbklj4xx5v7gtr23mxypjc580000gp/T/ipykernel_20265/2488641057.py:61: RuntimeWarning: coroutine 'get_next_questions' was never awaited\n",
      "  suggestions = await get_next_questions(messages[-1]['content'])\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "response = await client.chat.completions.create(\n",
    "    model=\"grok-2-1212\",\n",
    "    messages=messages,\n",
    "    tools=tools_definition,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "fn_call_in_progress = False\n",
    "fn_results = []\n",
    "\n",
    "async for chunk in response:\n",
    "\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for tool_call in chunk.choices[0].delta.tool_calls:\n",
    "\n",
    "            fn_name = tool_call.function.name\n",
    "            fn_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "\n",
    "            result = tools_map[fn_name](**fn_args)\n",
    "\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": json.dumps(result),\n",
    "                    \"tool_name\": fn_name,\n",
    "                    \"tool_call_id\": tool_call.id  # tool_call.id supplied in Grok's response\n",
    "                }\n",
    "            )\n",
    "\n",
    "if messages[-1]['role'] == 'tool':\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"grok-2-1212\",\n",
    "        messages=messages,\n",
    "        tools=tools_definition,\n",
    "        tool_choice=\"auto\",\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    async for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            messages[-1]['content'] += chunk.choices[0].delta.content \n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)  \n",
    "\n",
    "if messages[-2]['role'] == 'tool' and messages[-1]['role'] == 'assistant':\n",
    "    print(\"\\n\")\n",
    "    suggestions = await get_next_questions(messages[-1]['content'])\n",
    "    print('suggestions', suggestions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    'role': 'user',\n",
    "    'content': 'what is diabetes ?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
